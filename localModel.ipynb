{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfe5f8c3",
   "metadata": {},
   "source": [
    "## Local Model Testing to Prompt Engineer our Movie Reccomender\n",
    "Here we are loading in our models locally to try and optimize our prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813e5589",
   "metadata": {},
   "source": [
    "Now We'll load our models in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1dd7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\Dropbox\\Documents\\WPI\\25-26\\C Term\\MLOps\\movie-mood-recomender\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pipelines:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Getting pipelines:\")\n",
    "liquid = pipeline(\"text-generation\", model=\"LiquidAI/LFM2.5-1.2B-Instruct\",return_full_text=False)\n",
    "Qwen = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-0.5B-Instruct\",return_full_text=False)\n",
    "Geilim = pipeline(\"text-generation\", model=\"NoesisLab/Geilim-1B-Instruct\",trust_remote_code=True,return_full_text=False)\n",
    "# Llama = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ec0d3",
   "metadata": {},
   "source": [
    "Now it's time to set up our prompts. We are using langchain + Pydantic to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b695157e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='The Great Gatsby' genre='Fiction' reason='The Great Gatsby is a classic novel about love, loss, and ambition that has inspired countless filmmakers to tell its story.'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define the JSON schema we want model outputs to conform to\n",
    "class MovieRecommendation(BaseModel):\n",
    "    \"\"\"A movie recommendation based on mood.\"\"\"\n",
    "    title: str = Field(description=\"The title of the recommended movie\")\n",
    "    genre: str = Field(description=\"The genre of the movie\")\n",
    "    reason: str = Field(description=\"Brief explanation of why this movie matches the mood\")\n",
    "\n",
    "# Wrap a transformer pipeline with LangChain\n",
    "# IMPORTANT: return_full_text=False so only the generated text is returned, not the prompt\n",
    "Qwen.return_full_text = False\n",
    "llm = HuggingFacePipeline(pipeline=Qwen)\n",
    "\n",
    "# Parser that validates model output against the Pydantic schema\n",
    "parser = PydanticOutputParser(pydantic_object=MovieRecommendation)\n",
    "\n",
    "# Simpler prompt that small models can follow more reliably\n",
    "prompt = PromptTemplate(\n",
    "    template=(\n",
    "        \"Recommend a movie based on the user's query. \"\n",
    "        \"Respond with ONLY a JSON object with these keys: \"\n",
    "        \"\\\"title\\\" (movie title), \\\"genre\\\" (movie genre), \\\"reason\\\" (why it fits).\\n\\n\"\n",
    "        \"Query: {query}\\n\\n\"\n",
    "        \"JSON:\"\n",
    "    ),\n",
    "    input_variables=[\"query\"],\n",
    ")\n",
    "\n",
    "# Chain: prompt -> LLM -> parse & validate JSON\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"query\": \"Recommend a movie for someone feeling adventurous\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0553058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
